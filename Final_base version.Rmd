---
title: "Text Analytics Group Assignment - Team 9"
author: "Fangzhou Su, Junqi Liu, Jos√© Pastor, Mingwei Hou"
date: "22/03/2020"
output:
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    number_sections: yes
    toc: yes
  pdf_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

#Load Libraries and setup

```{r results='hide', message=FALSE, warning=FALSE}

rm(list=ls())
options(warn=-1)
options(message=FALSE)
options(warning=FALSE)
options(stringsAsFactors = FALSE)

library(rvest)
library(stringr)
library(dplyr)
library(RSQLite)
library(readr)
library(gutenbergr)
library(tidytext)
library(forcats)
library(ggplot2)
library(rJava)
library(qdap)
library(SnowballC)
library(tm)
library(tidyr)
library(igraph)
library(ggraph)
library(textcat)
library(stringdist)
library(factoextra)
library(udpipe)
library(maps)
library(mapdata)
library(sp)
library(data.table)
library(stringi)
library(edgar)
library(sentimentr)
library(reshape2)
library(remotes)
library(FactoMineR)
library(ggrepel)
library(mlr)
library(httr)
library(stargazer)
library(SentimentAnalysis)
library(lubridate)
library(ggmap)
library(gbm)
library(wordcloud)
register_google(key = "AIzaSyA8815okVHtnbV4gc1O9RDIHwFpxJafIuY")

#### Parallel Cores

library(parallel)
library(doParallel)
numCores <- detectCores()
registerDoParallel(numCores)

#### Stop Words

data("stop_words")
set.seed(1234)

```


# GETTING THE DATA
## Merging and download
>We first downloaded the relevant data files for each city and 
>combined them in a relational schema.  

```{r warning = FALSE}

sqlite <- dbDriver("SQLite")
#!!change the path to where the database is located!!
con <- dbConnect(sqlite,"airbnb2cities.db")
dbListTables(con) #Display tables
listing <- data.frame(dbFetch(dbSendQuery(con, "SELECT * FROM listings")))
reviews <- data.frame(dbFetch(dbSendQuery(con, "SELECT * FROM reviews")))
#calendar <- data.frame(dbFetch(dbSendQuery(con, "SELECT * FROM calendar")))

```

## File preprocessing

```{r warning = FALSE}
# Before doing anything lets tidy up the column names 
colnames(listing) <- gsub(" ","_",tolower(colnames(listing)))
colnames(listing) <- gsub("/","_",colnames(listing))

colnames(reviews) <- gsub(" ","_",tolower(colnames(reviews)))
colnames(reviews) <- gsub("/","_",colnames(reviews))

#colnames(calendar) <- gsub(" ","_",tolower(colnames(calendar)))
#colnames(calendar) <- gsub("/","_",colnames(calendar))

text_preprocessing <- function(x){
  x <- iconv(x, "latin1", "ASCII", "") %>% tolower() %>% replace_contraction() %>% removePunctuation %>% rm_stopwords(Top100Words, separate = F)
  x <- gsub('[[:digit:]]+',' ', x)
  return(x)
}

listing$readabilitytext <- listing$description
reviews$comments <- text_preprocessing(reviews$comments)
listing$description <- text_preprocessing(listing$description)

airbnb_df <- left_join(reviews, listing, by=c("listing_id"))


airbnb_df$review_length_chars <- nchar(airbnb_df$comments)

airbnb_df <- airbnb_df %>% 
  filter(review_length_chars>144) %>%  
  filter(review_length_chars<1000)

```

## Plot histogram of length of characters
```{r warning = FALSE}
hist(airbnb_df$review_length_chars,breaks = 200,main = "Review Length(All) -Right trim to 1000")
```

## Remove listings that have less than 5 reviews 

```{r}
to_remove_listing_ids <- airbnb_df %>% 
  group_by(listing_id) %>% 
  summarise(total = n()) %>% 
  filter(total<=5) %>% pull(listing_id)

airbnb_df <- airbnb_df %>% 
  filter(!(listing_id %in% to_remove_listing_ids))



```

# Part A a,b
## Check Bi-grams and N-grams
> Define functions

```{r}

#Define functions
count_grams <- function(dataset, n = 2) {
  if (n == 2){dataset %>%
      unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
      separate(bigram, c("word1", "word2"), sep = " ") %>%
      filter(!word1 %in% stop_words$word,
             !word2 %in% stop_words$word) %>%
      count(word1, word2, sort = TRUE)
  } else {
    dataset %>%
      unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
      separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
      filter(!word1 %in% stop_words$word,
             !word2 %in% stop_words$word,
             !word3 %in% stop_words$word) %>%
      count(word1, word2, word3, sort = TRUE)
  }
  
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

separate_grams <- function(grams, n, min_f){
  if (n == 2){
    grams %>%
      filter(n > min_f,
             !str_detect(word1, "\\d"),
             !str_detect(word2, "\\d"))
  } else if (n == 3){
    grams %>%
      filter(n > min_f,
             !str_detect(word1, "\\d"),
             !str_detect(word2, "\\d"),
             !str_detect(word3, "\\d"))
  }  
}

```

> Review Bi Grams

```{r}

df <- data.frame(listing_id = listing$listing_id, text = listing$description)
min_f <-  350
n = 2

separate_grams(count_grams(df, n), n, min_f) %>%
  visualize_bigrams()
```

> Review Tri Grams

```{r}

df <- data.frame(listing_id = listing$listing_id, text = listing$description)
min_f <-  150
n = 3

separate_grams(count_grams(df, n), n, min_f) %>%
  visualize_bigrams()
```

## Analyze word frequencies per aggregation categories

> Define Functions

```{r}
word_frequency <- function(df, vec_stopwords, character_removal, top_n, col_n){
  df_words <- df %>%
    unnest_tokens(word, text) %>%
    count(aggregation, word, sort = TRUE)
  
  stopwords <- tibble(word = vec_stopwords)
  
  df_words <- anti_join(df_words, stopwords, 
                        by = "word")
  
  plot_df <- df_words %>%
    bind_tf_idf(word, aggregation, n) %>%
    mutate(word = str_remove_all(word, character_removal)) %>%
    group_by(aggregation) %>% 
    top_n(top_n, tf_idf) %>%
    ungroup() %>%
    mutate(word = reorder_within(word, tf_idf, aggregation)) %>%
    mutate(author = factor(aggregation, levels = unique(df$aggregation)))
  
  return(plot_df)
}

visualize_frequency <- function(plot_df){
  ggplot(plot_df, aes(word, tf_idf, fill = aggregation)) +
    geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "tf-idf") +
    facet_wrap(~aggregation, ncol = col_n, scales = "free") +
    coord_flip() +
    scale_x_reordered()
}

vec_stopwords = c(c(airbnb_df$city_typed, "cm",
                    "ab", "_k", "_k_", "_x"), stop_words$word)
character_removal = c("_")

```

## Review the aggregation per Room Type

```{r}
#Define the dataframe for room type
text <- 'comments'
aggregation <- 'room_type'
vec_subset <- c(text, aggregation)
df <- airbnb_df[,c(text, aggregation)] %>% 
  setnames(old = vec_subset, new = c('text','aggregation'))

top_n <- 5 #Top number for words
col_n <- 2 #Two columns in final wrap

plot_df <- word_frequency(df, vec_stopwords, character_removal, top_n, col_n)
visualize_frequency(plot_df)
```

```{r}

plot_df$word <- data.frame(str_split_fixed(plot_df$word, "___", 2))$X1
wordcloud(words = plot_df$word, freq = plot_df$n, min.freq = 1,
          max.words=1000, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

## Review the aggregation per Property Type

```{r}
#Define the dataframe for property type
text <- 'comments'
aggregation <- 'property_type'
vec_subset <- c(text, aggregation)
df <- airbnb_df[,c(text, aggregation)] %>% 
  setnames(old = vec_subset, new = c('text','aggregation'))

top_n <- 5 #Top number for words
col_n <- 3 #Two columns in final wrap

plot_df <- word_frequency(df, vec_stopwords, character_removal, top_n, col_n)
visualize_frequency(plot_df)


```

```{r}
plot_df$word <- data.frame(str_split_fixed(plot_df$word, "___", 2))$X1
wordcloud(words = plot_df$word, freq = plot_df$n, min.freq = 1,
          max.words=1000, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

# Part A c 1
##Data pre-processing

```{r}
#build dataframe
readability_data <- listing %>%
  select(listing_id,readabilitytext,price,review_scores_rating)

#change listing_id to factor 
readability_data$listing_id <- as.factor(readability_data$listing_id)

#data pre-processing
readability_data$readabilitytext <- iconv(readability_data$readabilitytext, "latin1", "ASCII", "")
readability_data$readabilitytext <- gsub("[^ -~]", "", readability_data$readabilitytext)
readability_data <- na.omit(readability_data)
```

##Get flesch_kincaid score

```{r results='hide', message=FALSE, warning=FALSE}

readability_flesch_kincaid <- flesch_kincaid(readability_data$readabilitytext)
readability_outcome <- readability_flesch_kincaid$Counts
readability_outcome$flesch_kincaid_score <- 206.835 - readability_outcome$word.count/readability_outcome$tot.n.sent*1.015-84.6*readability_outcome$syllable.count/readability_outcome$word.count

readability_outcome$readabilitytext <- readability_outcome$text.var
readability <- inner_join(readability_outcome,readability_data)
na.omit(readability)

readability$price <- as.numeric(gsub("\\$","",readability$price))

#Remove outlinears
readability_wooutlinear <- readability %>% 
  filter(flesch_kincaid_score > 30 & flesch_kincaid_score < 130 & price < 625)



```
##Linear Regression
```{r}
readability_linear_regression_flesch_kincaid <- lm(review_scores_rating ~ flesch_kincaid_score,data =readability_wooutlinear)
summary(readability_linear_regression_flesch_kincaid)

```


##Plotting result
```{r message=FALSE, warning=FALSE}
ggplot(readability_wooutlinear) + geom_point(aes(x = flesch_kincaid_score,y = price)) + geom_abline(intercept =  96.523068,slope =-0.035117 ,color = "red")

ggplot(readability_wooutlinear) + geom_bin2d(aes(x = flesch_kincaid_score,y = price))


difficult <- readability_wooutlinear$flesch_kincaid_score <= 50
fairly_difficult <- readability_wooutlinear$flesch_kincaid_score <= 60 & readability_wooutlinear$flesch_kincaid_score >50
plain_english <- readability_wooutlinear$flesch_kincaid_score <= 70 & readability_wooutlinear$flesch_kincaid_score >60
fairly_easy <- readability_wooutlinear$flesch_kincaid_score <= 80 & readability_wooutlinear$flesch_kincaid_score >70
easy <- readability_wooutlinear$flesch_kincaid_score > 80

readability_wooutlinear$difficulty <- "1"
readability_wooutlinear$difficulty[difficult] <- "difficult"
readability_wooutlinear$difficulty[fairly_difficult] <- "fairly_difficult"
readability_wooutlinear$difficulty[plain_english] <- "plain_english"
readability_wooutlinear$difficulty[fairly_easy] <- "fairly_easy"
readability_wooutlinear$difficulty[easy] <- "easy"

readability_wooutlinear$difficulty <- as.factor(readability_wooutlinear$difficulty)


dif <- ggplot(readability_wooutlinear[readability_wooutlinear$difficulty == "difficult",]) + geom_histogram(aes(x = price)) + labs(x="Price", y="Number of Property",subtitle = "Price Allocation for Properties with descriptions difficult to read") + xlim(0,700)
fdif <- ggplot(readability_wooutlinear[readability_wooutlinear$difficulty == "fairly_difficult",]) + geom_histogram(aes(x = price)) + labs(x="Price", y="Number of Property",subtitle = "Price Allocation for Properties with descriptions fairly difficult to read") + xlim(0,700)
pe <- ggplot(readability_wooutlinear[readability_wooutlinear$difficulty == "plain_english",]) + geom_histogram(aes(x = price)) + labs(x="Price", y="Number of Property",subtitle = "Price Allocation for Properties with descriptions only uses plain language") + xlim(0,700)
fe <- ggplot(readability_wooutlinear[readability_wooutlinear$difficulty == "fairly_easy",]) + geom_histogram(aes(x = price)) + labs(x="Price", y="Number of Property",subtitle = "Price Allocation for Properties with descriptions fairly easy to read") + xlim(0,700)
easy <- ggplot(readability_wooutlinear[readability_wooutlinear$difficulty == "easy",]) + geom_histogram(aes(x = price)) + labs(x="Price", y="Number of Property",subtitle = "Price Allocation for Properties with descriptions easy to read") + xlim(0,700)

dif
fdif
pe
fe
easy

#We use linear regression to check the linear relationship of the price and the readability of property description. We also divide the readability into 5 levels ----- difficult, farly difficult, plain english, fairly eaasy and easy to show the price allocation under each catogary. It shows that the as the description gets more and more difficult to read, the property's price will slightly drop. And the histograms show the same pattern. So the property owners should keep the descriptions simple and neat for a better price.
```

# Part A c 2
##Data pre-processing

```{r  , results="hide",message=FALSE, warning=FALSE}

#remove automated posting
reviewnamemention <- reviews %>%
  select(listing_id,comments) %>%
  filter(!str_detect(comments,"this is an automated posting")) %>%
  filter(!str_detect(comments,"host canceled reservation"))
#remove revires that is too short
reviewnamemention$length <- nchar(reviewnamemention$comments)
reviewnamemention <- reviewnamemention %>%
  filter(length >=10)

#remove property with less than 10 reviews 
reviewcounttotal <- reviewnamemention %>%
  group_by(listing_id) %>%
  summarise(time = n()) %>%
  filter(time >=10)

mentionnames <- reviewnamemention %>%
  filter(listing_id %in% reviewcounttotal$listing_id)

names <- listing %>%
  select(listing_id,host_name)

detectmentionnames <- left_join(mentionnames,names)

detectmentionnames <- na.omit(detectmentionnames)

detectmentionnames$host_name <- text_preprocessing(detectmentionnames$host_name)
detectmentionnames <- na.omit(detectmentionnames)
```

##detect the whether host's name is mentioned
```{r results="hide",message=FALSE, warning=FALSE}

mentioned <-  vector()

for (i in 1:length(detectmentionnames$listing_id)){
  exist <- str_detect(detectmentionnames$comments[i],detectmentionnames$host_name[i])
  mentioned <- append(mentioned,exist)
}


detectmentionnames$mentioned <- mentioned


mentionrate <- detectmentionnames %>%
  group_by(listing_id) %>%
  summarise(mentionrate = mean(mentioned))


listing_neat <- listing %>%
  select(listing_id,price,review_scores_rating)

listing_mention <- left_join(listing_neat,mentionrate)
listing_mention <- na.omit(listing_mention)

#remove outliears
listing_mention_wooutlinear <- listing_mention %>% 
  filter(mentionrate > 0 & mentionrate < 1)



```

##analysis and result display 

```{r warning=FALSE}


#linear regression
mentionratelm <- lm(review_scores_rating ~ mentionrate,data = listing_mention_wooutlinear) 
summary(mentionratelm)

#plot
ggplot(listing_mention_wooutlinear) + geom_jitter(aes(x = mentionrate,y = review_scores_rating)) + geom_abline(intercept=92.7488,slope=5.0701,color = "red")


ggplot(listing_mention_wooutlinear) + geom_bin2d(aes(x = mentionrate,y = review_scores_rating),bins = 40)

#We calculated the name mention rate in the review for all the properties and it seems that as the  mention rate increases, the review score will also increase, indicating that customers mentioning prperty owner's name usually tend to be satisfying customers.
```

# Part A d
##Data pre-processing

```{r results="hide",message=FALSE, warning=FALSE}

polaritydata <- listing %>%
  select(listing_id,description)

polaritydata$description <- gsub('[0-9]+', '', polaritydata$description)

polaritydata <- na.omit(polaritydata)
```

##calculate polarity
```{r results="hide",message=FALSE, warning=FALSE}

polaritydata_tokenised <- polaritydata %>%
  unnest_tokens(word, description)


data(stop_words)
polaritydata_cleaned <- anti_join(polaritydata_tokenised,stop_words)

bing <- get_sentiments(lexicon = "bing")  
pos_neg <- polaritydata_cleaned %>% inner_join(bing)

pos_neg <- pos_neg %>% group_by(listing_id) %>% count(sentiment) %>% spread(sentiment,n,fill=0)%>% mutate(polarity = positive - negative) 


polarityprice <- listing %>%
  select(listing_id,price,review_scores_rating) %>%
  left_join(pos_neg) %>%
  na.omit()


polarityprice$price <- as.numeric(str_replace(polarityprice$price,"\\$",""))
polarityprice <- na.omit(polarityprice)

#remove outlinears
polarityprice_wooutlinear <- polarityprice %>% 
  filter(polarity > -5 & polarity < 25)


```

##analysis and result display 
```{r}

ggplot(polarityprice_wooutlinear) + geom_jitter(aes(x = polarity,y = price))  + geom_abline(intercept=133.356 ,slope= 2.535 ,color = "red")

ggplot(polarityprice_wooutlinear) + geom_bin2d(aes(x = polarity,y = price,bins = 50))


polaritylm <- lm(price ~ polarity,data = polarityprice_wooutlinear) 
summary(polaritylm)

#The result shows that property with more positive desriptions tend to have higher price. 
```




# Part B

## Sentiment Analysis

## Check which sentiments are the most prominent in the dataset

```{r}
plot_states_affective <- function(df){
  df%>% 
    unnest_tokens(word,text) %>% 
    anti_join(stop_words) %>% 
    inner_join(get_sentiments("nrc")) %>%
    filter(!(sentiment %in% c("positive","negative"))) %>% 
    group_by(sentiment) %>% 
    summarise(Total=n()) %>% 
    ggplot(.,aes(x=reorder(sentiment,-Total),y=Total,fill=sentiment))+geom_bar(stat = "identity") + xlab("Affective States")
}

plot_scatter_affective <- function(df){
  df %>% 
    unnest_tokens(word,text) %>% 
    anti_join(stop_words) %>% 
    inner_join(get_sentiments("nrc")) %>%
    group_by(id,sentiment) %>% 
    summarise(total_sentiment = n()) %>% 
    left_join(df%>% 
                unnest_tokens(word,text) %>% 
                anti_join(stop_words) %>% 
                inner_join(get_sentiments("nrc")) %>%
                group_by(id) %>% 
                summarise(total_words =n())) %>% 
    mutate(total_sentiment=total_sentiment/total_words)  %>% 
    acast(id~sentiment,
          value.var = "total_sentiment",fill=0) %>%
    as.data.frame() -> sent_transform
  p <- FactoMineR::CA(sent_transform)
  res_df_1 <- data.frame(p$row$coord)
  res_df_2 <- data.frame(p$col$coord)
  res_df_2 <- cbind(sentiment = rownames(res_df_2), res_df_2)
  rownames(res_df_2) <- 1:nrow(res_df_2)
  
  p <- ggplot(data = res_df_1, aes(x = Dim.1, y = Dim.2)) + geom_hex(bins = 50) +   scale_fill_continuous(type = "viridis")
  p <- p + geom_point(data = res_df_2, aes(x=Dim.1, y=Dim.2, color = "red")) + geom_label_repel(data = res_df_2, aes(x=Dim.1, y=Dim.2, label = sentiment, color = "red"), size = 5)
  p
}
```

## Apply and check

```{r message=FALSE, warning=FALSE}
text <- 'comments'
aggregation <- 'listing_id'
vec_subset <- c(text, aggregation)
df <- airbnb_df[,c(text, aggregation)] %>% 
  setnames(old = vec_subset, new = c('text','id'))

plot_states_affective(df)
```

```{r message=FALSE, warning=FALSE}
plot_scatter_affective(df)
```

## Analyze relationship between Price and Sentiment 

## First we need to generate the sentiment data

```{r message=FALSE, warning=FALSE}

all_data_df <- airbnb_df
split_size <- 10000
tokens_list <- split(all_data_df, 
                     rep(1:ceiling(nrow(all_data_df)
                                   /split_size), 
                         each=split_size,
                         length.out=nrow(all_data_df)))

tokens_all <- data.frame()
for(i in 1:length(tokens_list)){
  tokens_h <- tokens_list[[i]] %>% 
    unnest_tokens(word,comments) %>%
    count(word,listing_id) %>%
    anti_join(stop_words)
  tokens_all <- bind_rows(tokens_all,tokens_h)
}

tokens_all$token_length <- nchar(tokens_all$word)

tokens_all <- tokens_all %>% 
  filter(token_length>2)

tokens_all <- tokens_all %>% 
  filter(token_length<=15)

tokens_all_tf_idf <- tokens_all %>% 
  bind_tf_idf(word,listing_id,n) %>% 
  filter(tf_idf<0.05) %>% 
  filter(tf_idf>0.001)

listing_id_price <- listing %>% 
  select(listing_id,price) %>% 
  mutate(price = as.numeric(gsub("\\$","",price)))

all_data_df %>% 
  group_by(listing_id) %>% 
  summarise(avg_rating = mean(review_scores_rating))->listing_id_avg_rating

# Bing Liu - Sentiment Dictionary 
tokens_all %>% inner_join(get_sentiments("bing")) %>%
  count(sentiment,listing_id) %>% spread(sentiment,n) %>%
  mutate(bing_liu_sentiment = positive-negative) %>%
  select(listing_id,bing_liu_sentiment) -> bing_liu_sentiment_listing

bing_liu_sentiment_listing <- bing_liu_sentiment_listing %>%
  left_join(listing_id_price)

bing_liu_sentiment_listing

# NRC Dictionary 

tokens_all %>% inner_join(get_sentiments("nrc")) %>% 
  count(sentiment,listing_id) %>% 
  spread(sentiment,n)  -> emotions_nrc

emotions_nrc <- emotions_nrc %>% 
  left_join(listing_id_price) %>% 
  mutate(sentiment_nrc = positive-negative)

emotions_nrc

# Afinn Dictionary

tokens_all %>% inner_join(get_sentiments("afinn")) %>% 
  group_by(listing_id) %>% 
  summarise(sentiment_affin = sum(value)) -> sentiment_affin

sentiment_affin <- sentiment_affin %>% 
  left_join(listing_id_price)

sentiment_affin

# Loughran Dictionary

tokens_all %>% inner_join(get_sentiments("loughran")) %>% 
  count(sentiment,listing_id) %>% 
  spread(sentiment,n) -> sentiments_loughran

sentiments_loughran <- sentiments_loughran %>% 
  left_join(listing_id_price) %>% 
  mutate(sentiment_loughran = positive-negative) %>% 
  select(listing_id,price,sentiment_loughran)

sentiments_loughran

all_together_sentiments <- bing_liu_sentiment_listing %>% 
  left_join(emotions_nrc) %>%
  left_join(sentiment_affin) %>%
  left_join(sentiments_loughran) %>% 
  na.omit()

all_together_sentiments


```

## Run regressions to check which sentiment is more significant

```{r}
# Bing Liu
model1 <- lm(log(price)~bing_liu_sentiment, 
             data=all_together_sentiments)

# NRC affection 
model2 <- lm(log(price)~sentiment_nrc,
             data=all_together_sentiments)

#Afinn 
model3 <- lm(log(price)~sentiment_affin,
             data=all_together_sentiments)

model4 <- lm(log(price)~sentiment_loughran, 
             data=all_together_sentiments)

stargazer(model1,model2,model3,model4,type = "text")
```

## Since the results are not convincing we try a ML approach using MLR and gradient boosting machines, 3 fold cross validation 

```{r warning=FALSE}

data = all_together_sentiments[,! names(all_together_sentiments) %in% "listing_id", drop = F]

smp_size <- floor(0.75 * nrow(data))

set.seed(123)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)

train <- data[train_ind, ]
test <- data[-train_ind, ]

label <- "price"
task = makeRegrTask(id = "bh", data = train, target = label)
lrn = makeLearner("regr.gbm", par.vals = list(n.trees =
                                                500, interaction.depth = 10))

rdesc = makeResampleDesc("CV", iters = 3)

validation_result <- mlr::resample(
  learner = lrn,
  task = task,
  resampling = rdesc,
  keep.pred = TRUE
)

validation_result

model <- train(lrn, task)
preds <- predict(model, newdata = test)

test_predicted <- bind_cols(test, preds$data)

```

## The prediction results as a percentage of variation of the price are the following

```{r}
result_df <- test_predicted[, c('price', 'response')]

plot_colorByDensity = function(x1,x2,
                               ylim=c(min(x2),max(x2)),
                               xlim=c(min(x1),max(x1)),
                               xlab="",ylab="",main="") {
  
  df <- data.frame(x1,x2)
  x <- densCols(x1,x2, colramp=colorRampPalette(c("black", "white")))
  df$dens <- col2rgb(x)[1,] + 1L
  cols <-  colorRampPalette(c("#000099", "#00FEFF", "#45FE4F","#FCFF00", "#FF9400", "#FF3100"))(256)
  df$col <- cols[df$dens]
  plot(x2~x1, data=df[order(df$dens),], 
       ylim=ylim,xlim=xlim,pch=20,col=col,
       cex=2,xlab=xlab,ylab=ylab,
       main=main)
}

x <- 100*(result_df$price - result_df$response) / result_df$price
plot_colorByDensity(1:length(x), x)
```

## The variable importance for the prediction is as follows

```{r}
var_importance <- generateFeatureImportanceData(
  task,
  method = "permutation.importance",
  lrn,
  colnames(data)[!colnames(data) %in% label],
  nmc = 50L,
  local = FALSE
)

var_importance <- data.frame(t(data.frame(var_importance$res)))
var_importance$var <- colnames(data)[!colnames(data) %in% label]

ggplot(var_importance, aes(x=reorder(var,mse), y=mse)) +
  geom_bar(stat='identity') +
  coord_flip()
```

## For an interesting visualization of the relationship of price and sentiment of "Trust", which we saw is the most important, we use maps:

```{r message=FALSE, warning=FALSE}

airbnb_df_db <- left_join(all_together_sentiments, listing[,c("listing_id", "longitude", "latitude")], by=c("listing_id"))

p <- ggmap(get_map("Amsterdam", maptype="roadmap", zoom=18, messaging = FALSE, markers = FALSE))
p <- p + geom_point(data=airbnb_df_db, aes(x=airbnb_df_db$longitude, y=airbnb_df_db$latitude, color = log(trust), size = 5, aplha = .5))+   scale_color_continuous(type = "viridis")

q <- ggmap(get_map("Amsterdam", maptype="roadmap", zoom=18, messaging = FALSE, markers = FALSE))
q <- q + geom_point(data=airbnb_df_db, aes(x=airbnb_df_db$longitude, y=airbnb_df_db$latitude, color = log(price), size = 5, aplha = .5))+   scale_color_continuous(type = "viridis")

require(gridExtra)
grid.arrange(p, q, ncol=2)

```


#Part C

##=================         Get Data           =================##
##-----------------------------------------------------------------##
After downloading the data and storing them to the database, we get data from the database everytime.
```{r getdata, warning=FALSE}
#con <- dbConnect(SQLite(), "airbnb2cities.db")
listing_partc <- dbSendQuery(con, "SELECT * FROM listings")
listing_partc <- dbFetch(listing_partc)
reviews_partc <- dbSendQuery(con, "SELECT * FROM reviews")
reviews_partc <- dbFetch(reviews_partc)

nrow(listing_partc)
nrow(reviews_partc)
```

##=================    Generate Dataframe      =================##
##-----------------------------------------------------------------##
Create the dataframe we will use in further analysis.
```{r generatedataframe, warning=FALSE}
# to be able to merge we need to have a joint field 
airbnb_df_partc <- left_join(reviews_partc, listing_partc, by=c("listing_id"))
airbnb_df_partc <-airbnb_df[airbnb_df$property_type %in% c("Apartment", "House", "Townhouse", "Loft", "Boat"),]
```

##====================    4. Part C      ==========================##
##-----------------------------------------------------------------##



```{r partcgeneratedataframe}
# For Part C, we just need review_id, comments, review_scores_rating, listing_id, price, and city_typed from original dataframe
data_for_partc_2_cities <- airbnb_df_partc %>% 
  select(review_id,comments, review_scores_rating, listing_id, price, city_typed) %>% 
  mutate(price = as.numeric(gsub("\\$","",price)))
```
##4.1 Part C---Amsterdam

Question a: 

The entire document for customer review comments can be composed of a mixture of 37 topics. As can be seen from the topic proportion table, the most frequent topics appearing in the reviews are those concerning the hospitality of host, followed by location of the property and its implication on transportation, and the special features of different rooms are the least frequent.

Question b: 

In terms of predictability, topics mentioning late-night service from the staff, luggage accessibility and room functions such as bathroom and kitchen are more closely linked with higher overall rating scores, while the lack of helpfulness from staff and additional service such as tourist recommendation are more likely resulting in lower rating scores.

Topics in comments of customer review do not show large difference in the pricing of the property.

```{r amsterdamdataframe}
city_index_amsterdam <- "amsterdam"
partc_amsterdam <- data_for_partc_2_cities[data_for_partc_2_cities$city_typed %in% city_index_amsterdam,]
```

```{r amsterdampreprocessing, eval=FALSE}
# Create a column using the number of characters
partc_amsterdam$review_length_chars <- nchar(partc_amsterdam$comments)

# Plot all
hist(partc_amsterdam$review_length_chars,breaks = 200,main = "Review Length(All)")

# Filter for minimum length of 144 and do a right trim to maximum 1000 chars 
partc_amsterdam <- partc_amsterdam %>% 
  filter(review_length_chars>144)
#hist(partc_amsterdam$review_length_chars,breaks = 200,main = "Review Length(All) -Left trim to 144")
partc_amsterdam <- partc_amsterdam %>% 
  filter(review_length_chars<1000)
#hist(partc_amsterdam$review_length_chars,breaks = 200,main = "Review Length(All) -Right trim to 1000")

# Remove those listings that have fewer than 5 reviews 
to_remove_listing_ids <- partc_amsterdam %>% 
  group_by(listing_id) %>% 
  summarise(total = n()) %>% 
  filter(total<=5) %>% pull(listing_id)

partc_amsterdam <- partc_amsterdam %>% 
  filter(!(listing_id %in% to_remove_listing_ids)) 

# Substitute numbers and punctuation marks with space
partc_amsterdam$comments <- gsub('[[:digit:]]+',' ', partc_amsterdam$comments)
partc_amsterdam$comments <- gsub('[[:punct:]]+',' ', partc_amsterdam$comments)
```


```{r amsterdamgeneratetokens, message=FALSE, warning=FALSE}
split_size <- 10000
tokens_list_amsterdam <- split(partc_amsterdam, 
                               rep(1:ceiling(nrow(partc_amsterdam)
                                             /split_size), 
                                   each=split_size,
                                   length.out=nrow(partc_amsterdam)))
tokens_all_amsterdam <- data.frame()
for(i in 1:length(tokens_list_amsterdam)){
  tokens_h <- tokens_list_amsterdam[[i]] %>% 
    unnest_tokens(word,comments) %>%
    count(word,review_id) %>%
    anti_join(stop_words)
  #print(i)
  tokens_all_amsterdam <- bind_rows(tokens_all_amsterdam,tokens_h)
}
```

```{r amsterdamtokensremove, message=FALSE, warning=FALSE}
# Calculate the token length and remove those with 1, 2, and 3 characters 
tokens_all_amsterdam$token_length <- nchar(tokens_all_amsterdam$word)
tokens_all_amsterdam %>% group_by(token_length) %>% summarise(total =n())

tokens_all_amsterdam <- tokens_all_amsterdam %>% 
  filter(token_length>3)

tokens_all_amsterdam %>% group_by(token_length) %>% 
  summarise(total =n()) %>% 
  arrange(desc(token_length))

# Remove those with more than 15 characters
tokens_all_amsterdam <- tokens_all_amsterdam %>% 
  filter(token_length<=15)

# Calculate tf-idf using the review as a document 
tokens_all_tf_idf_amsterdam <- tokens_all_amsterdam %>% 
  bind_tf_idf(word,review_id,n)
```

```{r amsterdamudpipe, message=FALSE, warning=FALSE}
#language <- udpipe_download_model(language="english",overwrite = F)
ud_model <- udpipe_load_model("english-ewt-ud-2.4-190531.udpipe")

partc_amsterdam_backup <- partc_amsterdam
tokens_all_tf_idf_amsterdam_backup <- tokens_all_tf_idf_amsterdam

# Parallelization
annotated_reviews_all_amsterdam <- data.frame()
split_size <- 100

for_pos_list_amsterdam <- split(tokens_all_tf_idf_amsterdam, 
                                rep(1:ceiling(nrow(tokens_all_tf_idf_amsterdam)
                                              /split_size), 
                                    each=split_size,
                                    length.out=nrow(tokens_all_tf_idf_amsterdam)))

for(i in 1:length(for_pos_list_amsterdam)){
  udpipe_annotate(for_pos_list_amsterdam[[i]]$word,
                  doc_id = for_pos_list_amsterdam[[i]]$review_id,
                  object = ud_model) %>% 
    as.data.frame() %>% 
    filter(upos %in% c("NOUN","ADJ","ADV")) %>%
    select(doc_id,lemma) %>% 
    group_by(doc_id) %>% 
    summarise(annotated_comments = paste(lemma, collapse = " ")) %>% 
    rename(review_id = doc_id) -> this_annotated_reviews
  
  #print(paste(i,"from",length(for_pos_list_amsterdam)))
  annotated_reviews_all_amsterdam <- bind_rows(annotated_reviews_all_amsterdam,
                                     this_annotated_reviews)
}

```

```{r amsterdamstmexecutiondataframe, message=FALSE, warning=FALSE}
annotated_reviews_all_amsterdam$review_id <- as.integer(annotated_reviews_all_amsterdam$review_id)

annotated_reviews_all_amsterdam_backup <- annotated_reviews_all_amsterdam

# Combine tokens togther for quicker stm execution
annotated_reviews_all_amsterdam <- annotated_reviews_all_amsterdam %>% group_by(review_id) %>% summarise(tokens_combined = str_c(annotated_comments, collapse = " ")) %>% ungroup()

annotated_reviews_all_amsterdam %>% left_join(partc_amsterdam) -> partc_amsterdam

# price: some NAs included
partc_amsterdam <- partc_amsterdam %>% na.omit()

partc_amsterdam <- partc_amsterdam %>% select(review_id, tokens_combined, review_scores_rating, price)
```

```{r amsterdamstmexecution, message=FALSE, warning=FALSE}
library(stm)
processed_amsterdam <- textProcessor(partc_amsterdam$tokens_combined,
                           metadata = partc_amsterdam,
                           customstopwords = c("airbnb",city_index_amsterdam),
                           stem = F)

threshold_amsterdam <- round(1/100 * length(processed_amsterdam$documents),0)

out_amsterdam <- prepDocuments(processed_amsterdam$documents,
                     processed_amsterdam$vocab,
                     processed_amsterdam$meta,
                     lower.thresh = threshold_amsterdam)
 
library(Rtsne)
library(rsvd)
library(geometry)

# STM execution
# To find the optimal K value, using K=0 to generate automatically, credit to Nikos' tip
airbnbfit_amsterdam <- stm(documents = out_amsterdam$documents,
                 vocab = out_amsterdam$vocab,
                 K = 0,
                 prevalence =~ price+review_scores_rating,
                 max.em.its = 75, 
                 data = out_amsterdam$meta,
                 reportevery=3,
                 # gamma.prior = "L1",
                 sigma.prior = 0.7,
                 init.type = "Spectral")

topic_summary_amsterdam <- summary(airbnbfit_amsterdam)
```

After finishing stm execution, we generated 37 topics for Amsterdam
```{r amsterdamtopics, message=FALSE, warning=FALSE}

# summarise from the most frequent words to get the label for each topic

topic_labels_amsterdam <- c("easy transportation", "accessibility", "quick", "cleanliness", "helpful staff", "nearby facilities", "location", "home-like", "arrival welcome", "overall impression", "neighbourhood", "downtown", "central", "peaceful park", "breakfast", "museum", "friendly host", "convenient daily life", "value of money", "spacial", "room amenity", "answers questions", "late-night service", "beautiful", "local trip recommendation", "bike tour", "responsive", "terrace", "luggage friendly", "shopping", "stylish", "room functions", "train", "tourist attraction", "city feel", "window view", "wine")

# Prepare a table to put the topic proportions in
topic_proportions_amsterdam <- colMeans(airbnbfit_amsterdam$theta)

table_towrite_labels_amsterdam <- data.frame()

for(i in 1:length(topic_summary_amsterdam$topicnums)){
  
  row_here <- tibble(topicnum = topic_summary_amsterdam$topicnums[i],
                     topic_label = topic_labels_amsterdam[i],
                     proportion = 100*round(topic_proportions_amsterdam[i],4),
                     frex_words = paste(topic_summary_amsterdam$frex[i,1:7],
                                        collapse = ", "))
  table_towrite_labels_amsterdam <- rbind(row_here,table_towrite_labels_amsterdam)
}
table_towrite_labels_amsterdam %>% arrange(desc(proportion))

# Plot proportion relationships among each topic
plot(airbnbfit_amsterdam,custom.labels = topic_labels_amsterdam,main = "")
```

```{r amsterdamestimateeffect}
convergence_amsterdam <- as.data.frame(airbnbfit_amsterdam$theta)
colnames(convergence_amsterdam) <- paste0("topic",1:37)

effects_amsterdam <- estimateEffect(~review_scores_rating + price,
                          stmobj = airbnbfit_amsterdam,
                          metadata = out_amsterdam$meta)

# Plot effects corresponding to review_scores_rating
plot(effects_amsterdam, covariate = "review_scores_rating",
     topics = c(1:37),
     model = airbnbfit_amsterdam, method = "difference",
     cov.value1 = "100", cov.value2 = "64",
     xlab = "Low Rating ... High Rating",
     xlim = c(-0.005,0.005),
     main = "",
     custom.labels = topic_labels_amsterdam,
     labeltype = "custom")

# Plot effects corresponding to price
plot(effects_amsterdam, covariate = "price",
     topics = c(1:37),
     model = airbnbfit_amsterdam, method = "difference",
     cov.value1 = "810", cov.value2 = "20",
     xlab = "Low Price ... High Price",
     xlim = c(-0.005,0.005),
     main = "",
     custom.labels = topic_labels_amsterdam,
     labeltype = "custom")
```
##Part C Antwerp

Question a: 

The entire document for customer review comments of Antwerp can be composed of a mixture of 42 topics. As can be seen from the topic proportion table, the most frequent topics appearing in the reviews are the overal impression of the room, and the services included, while individual expectations from customers are the least frequent.

Question b: 

In terms of predictability, customer satisfaction is higher towards luxurious properties (loft), rooms that are convenient for everyday life, and those offering return offers, while difficulty in moving luggage, insufficient host hospitality and service and lack of style in the room are more likely resulting in lower rating scores.

In spite of a slightly higher chance of high price for helpful host and thorough service, topics in comments of customer review do not show large difference  in the pricing of the property.

```{r antwerpdataframe, message=FALSE, warning=FALSE}
city_index_antwerp <- "antwerp"
partc_antwerp <- data_for_partc_2_cities[data_for_partc_2_cities$city_typed %in% city_index_antwerp,]

```

```{r antwerppreprocessing, message=FALSE, warning=FALSE}
# Create a column using the number of characters
partc_antwerp$review_length_chars <- nchar(partc_antwerp$comments)

# Plot all
hist(partc_antwerp$review_length_chars,breaks = 200,main = "Review Length(All)")

partc_antwerp <- partc_antwerp %>% 
  filter(review_length_chars>144)

#hist(partc_antwerp$review_length_chars,breaks = 200,main = "Review Length(All) -Left trim to 144")

partc_antwerp <- partc_antwerp %>% 
  filter(review_length_chars<1000)

#hist(partc_antwerp$review_length_chars,breaks = 200,main = "Review Length(All) -Right trim to 1000")

# Remove those listings that have fewer than 5 reviews 
to_remove_listing_ids <- partc_antwerp %>% 
  group_by(listing_id) %>% 
  summarise(total = n()) %>% 
  filter(total<=5) %>% pull(listing_id)

partc_antwerp <- partc_antwerp %>% 
  filter(!(listing_id %in% to_remove_listing_ids)) 

# Substitute numbers and punctuation marks with space
partc_antwerp$comments <- gsub('[[:digit:]]+',' ', partc_antwerp$comments)
partc_antwerp$comments <- gsub('[[:punct:]]+',' ', partc_antwerp$comments)
```

```{r antwerpgeneratetokense, message=FALSE, warning=FALSE}
split_size <- 10000
tokens_list_antwerp <- split(partc_antwerp, 
                               rep(1:ceiling(nrow(partc_antwerp)
                                             /split_size), 
                                   each=split_size,
                                   length.out=nrow(partc_antwerp)))
tokens_all_antwerp <- data.frame()
for(i in 1:length(tokens_list_antwerp)){
  tokens_h <- tokens_list_antwerp[[i]] %>% 
    unnest_tokens(word,comments) %>%
    count(word,review_id) %>%
    anti_join(stop_words)
  #print(i)
  tokens_all_antwerp <- bind_rows(tokens_all_antwerp,tokens_h)
}

```

```{r antwerptokensclean, message=FALSE, warning=FALSE}
# Calculate the token length and remove those with 1, 2, and 3 characters 
tokens_all_antwerp$token_length <- nchar(tokens_all_antwerp$word)
tokens_all_antwerp %>% group_by(token_length) %>% summarise(total =n())

tokens_all_antwerp <- tokens_all_antwerp %>% 
  filter(token_length>3)

tokens_all_antwerp %>% group_by(token_length) %>% 
  summarise(total =n()) %>% 
  arrange(desc(token_length))

# Remove those with more than 15 characters
tokens_all_antwerp <- tokens_all_antwerp %>% 
  filter(token_length<=15)

# Calculate tf-idf using the review as a document 
tokens_all_tf_idf_antwerp <- tokens_all_antwerp %>% 
  bind_tf_idf(word,review_id,n)

```

```{r antwerpudpipe, message=FALSE, warning=FALSE}
ud_model <- udpipe_load_model("english-ewt-ud-2.4-190531.udpipe")

partc_antwerp_backup <- partc_antwerp
tokens_all_tf_idf_antwerp_backup <- tokens_all_tf_idf_antwerp

# Parallelization
annotated_reviews_all_antwerp <- data.frame()
split_size <- 100

for_pos_list_antwerp <- split(tokens_all_tf_idf_antwerp, 
                                rep(1:ceiling(nrow(tokens_all_tf_idf_antwerp)
                                              /split_size), 
                                    each=split_size,
                                    length.out=nrow(tokens_all_tf_idf_antwerp)))

for(i in 1:length(for_pos_list_antwerp)){
  udpipe_annotate(for_pos_list_antwerp[[i]]$word,
                  doc_id = for_pos_list_antwerp[[i]]$review_id,
                  object = ud_model) %>% 
    as.data.frame() %>% 
    filter(upos %in% c("NOUN","ADJ","ADV")) %>%
    select(doc_id,lemma) %>% 
    group_by(doc_id) %>% 
    summarise(annotated_comments = paste(lemma, collapse = " ")) %>% 
    rename(review_id = doc_id) -> this_annotated_reviews
  
  #print(paste(i,"from",length(for_pos_list_antwerp)))
  annotated_reviews_all_antwerp <- bind_rows(annotated_reviews_all_antwerp,
                                     this_annotated_reviews)
}

```

```{r antwerpstmexecutiondataframe, message=FALSE, warning=FALSE}

annotated_reviews_all_antwerp$review_id <- as.integer(annotated_reviews_all_antwerp$review_id)

annotated_reviews_all_antwerp_backup <- annotated_reviews_all_antwerp

# Combine tokens togther for quicker stm execution
annotated_reviews_all_antwerp <- annotated_reviews_all_antwerp %>% group_by(review_id) %>% summarise(tokens_combined = str_c(annotated_comments, collapse = " ")) %>% ungroup()

annotated_reviews_all_antwerp %>% left_join(partc_antwerp) -> partc_antwerp

# price: some NAs included
partc_antwerp <- partc_antwerp %>% na.omit()

partc_antwerp <- partc_antwerp %>% select(review_id, tokens_combined, review_scores_rating, price)
```

```{r antwerpstmexecution, message=FALSE, warning=FALSE}
library(stm)
processed_antwerp<- textProcessor(partc_antwerp$tokens_combined,
                           metadata = partc_antwerp,
                           customstopwords = c("airbnb",city_index_antwerp),
                           stem = F)

threshold_antwerp <- round(1/100 * length(processed_antwerp$documents),0)

out_antwerp <- prepDocuments(processed_antwerp$documents,
                     processed_antwerp$vocab,
                     processed_antwerp$meta,
                     lower.thresh = threshold_antwerp)

library(Rtsne)
library(rsvd)
library(geometry)
airbnbfit_antwerp <- stm(documents = out_antwerp$documents,
                 vocab = out_antwerp$vocab,
                 K = 0,
                 prevalence =~ price+review_scores_rating,
                 max.em.its = 75, 
                 data = out_antwerp$meta,
                 reportevery=3,
                 # gamma.prior = "L1",
                 sigma.prior = 0.7,
                 init.type = "Spectral")

topic_summary_antwerp <- summary(airbnbfit_antwerp)
```

After finishing stm execution, we generated 41 topics for Antwerp
```{r antwerptopics}
# summarise from the most frequent words to get the label for each topic

topic_labels_antwerp <- c("advice", "communication", "booking service", "location", "cozy", "warm welcome", "care", "nearby food", "bathroom", "window view", "historical value of city", "train access", "cleanliness", "spacious", "household items", "breakfast", "close to tourist sights", "overall experience", "prompt response", "parking", "terrace", "return offer", "transportation", "house", "helpful", "hospitality", "food recommendation", "loft", "value of money", "noise", "host", "responsive", "picture", "arrival service", "luggage unfriendly", "shopping", "room amenity", "foot trip", "safe", "family-like", "stylish", "leisure walk")

#Prepare a table to put the topic proportions in

topic_proportions_antwerp <- colMeans(airbnbfit_antwerp$theta)

table_towrite_labels_antwerp <- data.frame()
for(i in 1:length(topic_summary_antwerp$topicnums)){
  
  row_here <- tibble(topicnum= topic_summary_antwerp$topicnums[i],
                     topic_label = topic_labels_antwerp[i],
                     proportion = 100*round(topic_proportions_antwerp[i],4),
                     frex_words = paste(topic_summary_antwerp$frex[i,1:7],
                                        collapse = ", "))
  table_towrite_labels_antwerp <- rbind(row_here,table_towrite_labels_antwerp)
}
table_towrite_labels_antwerp %>% arrange(desc(proportion))


# Plot proportion relationships among each topic
plot(airbnbfit_antwerp,custom.labels = topic_labels_antwerp,main = "")
```

```{r antwerpestimateeffect}
convergence_antwerp <- as.data.frame(airbnbfit_antwerp$theta)
colnames(convergence_antwerp) <- paste0("topic",1:34)


effects_antwerp <- estimateEffect(~review_scores_rating + price,
                          stmobj = airbnbfit_antwerp,
                          metadata = out_antwerp$meta )

# Plot effects corresponding to review_scores_rating
plot(effects_antwerp, covariate = "review_scores_rating",
     topics = c(1:34),
     model = airbnbfit_antwerp, method = "difference",
     cov.value1 = "100", cov.value2 = "68",
     xlab = "Low Rating ... High Rating",
     xlim = c(-0.005,0.005),
     main = "",
     custom.labels = topic_labels_antwerp,
     labeltype = "custom")

# Plot effects corresponding to price
plot(effects_antwerp, covariate = "price",
     topics = c(1:34),
     model = airbnbfit_antwerp, method = "difference",
     cov.value1 = "480", cov.value2 = "10",
     xlab = "Low Price ... High Price",
     xlim = c(-0.005,0.005),
     main = "",
     custom.labels = topic_labels_antwerp,
     labeltype = "custom")
```


